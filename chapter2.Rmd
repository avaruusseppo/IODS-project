# Chapter 2 - Data analysis exercise

Fri 06 Nov 2020

### i) Loading a prepared data set and examine its contents.

Loading a prepared csv table, containing data from a learning study 
(International survey of Approaches to Learning, Kimmo Vehkalahti, fall 2016).
Metainformation: https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS3-meta.txt

```{r}
lrn14<-read.csv("data/learning2014.csv")
print(dim(lrn14))
str(lrn14)
```


A quick overview of the contents in this data:

This is a learning study on a statistics students, observing the different factors that might relate to a high or low exam score (points) for students of different ages and genders.

The students were interviewed about their attitude towards statistics, and different
motivational questions about their modes of learning. 

The mode-related questions are grouped in three different categories (deep, surface and strategic). The category modes are averages of the numeric questionnaire results.

By examining the structure, we can see that the data has 166 observations in 7 variable columns.

  * Three background attributes given from the informants: __attitude estimate, age and gender__.
  * Three pre-processed learning factors are: __surface, deep and strategic__.
  * Data "points" describe the exam __points__ or the output score. (Note! Zero exam point score rows are filtered out.)

### ii) An overview of the data

The **summary()** function for a data frame in R shows the ranges where the values of variables reside.

```{r}
# This show summaries of the variables in the data
summary(lrn14)
```

The learning mode parameters (*deep, stra, surf*) are averaged from a Likert style (1-5) questionnaire.
The *attitude* is the subjective meter of interest towards learning statistics.
The *age* is the student's age in years.

The *exam* points (used as an output variable in this study) vary from 7 to 33. 
By looking at the 1st and 3rd 
quantile in the summary output, 
the score density could be expected to fit in the Gaussian bell curve.

**Simple plot with linear fitting**

We'll try quickly a scatter plot with matching attitude to exam points.
This is quite much what the excel chart wizards do (but nothing more).

```{r}
# We need the 'ggplot2' graphics package and ggpairs (from GGally)
library(ggplot2)
library(GGally)

# A scatter plot of students attitude and exam points
qplot(attitude, points, col = gender, data = lrn14) + geom_smooth(method = "lm")
```

When we examine graphically the correlation of attitude to learning and the scores,
we can find a positive correlation. What we can see here is that the gender is not too much a factor in the course scores. A great attitude would predict a high score so we would expect a positive correlation.

As a second example, we might be especially interested how the different estimated learning modes affect the score. For instance, the strategic learning shows out promising. A positive correlation is shown as a linear model fitted towards the upper right-hand corner.

```{r}
# A scatter plot of students strategic approach on points, fitter by gender
qplot(stra, points, col = gender, data = lrn14) + geom_smooth(method = "lm")
```

Now, instead of examining a single variable's effect and distribution, we might
want to have a graphical overview of them all, with GGally

```{r}

ggpairs(lrn14, lower = list(combo = wrap("facethist", bins = 20)))
```


### iii) Fitting a linear model with function lm()

We continue building a linear model with multiple variables. This model is something that predicts the output variable (exam score) from input variables. 

After examining the matrix plot above,I choose three variables: **age, attitude** and **strategic approach**) as explanatory input variables. A regression model is fitted to these variables where exam points is the dependent (output) variable. 

Below is a summary of the fitted model and comment, with an interpretation of the R output.

```{r}
# create a regression model with multiple explanatory variables
my_model2 <- lm(points ~ attitude + stra + age, data = lrn14)
```

The parameter "*points ~ attitude + stra + age*" is a so-called *formula*. 
It states the dependent variable on the left-hand side of the tilde ("~") mark.
On the right-hand side are the examined coefficients are listed.
In this model, the coefficients are ought to be independent of each other.

```{r}
# print out a summary of the model
summary(my_model2)
```
The summary() function for a lm() model shows correlations and F-scores to the coefficients, which are practically the input variables for our model.

At first R outputs a summary of the weighted residuals distribution. 
These are the error residuals between the estimated and registered output values (points).

The first listed coefficient (intercept) is a baseline, that is a fixed the mean of the output.
The following coefficients are expected to affect the output at some correlation.
The coefficient correlation is the "slope" of the fitted linear estimate
for the effect of the input variable and output.

For each coefficient, R prints out a t-statistic (so-called Student's test) and a p-value, 
which reflects the probability of significant correlation between 
the coefficient and the output variable.
R pretty-prints nicely significance codes: E.g. "***" on a strong significance (p<0.001),  and "." on a smaller significance (0.05 < p < 0.1) .
(Usually most social studies use the 95 percent confidence rule as the baseline but we can accept this model for now.)

The F-statistic is the so-called test of equality of variances.
The smaller the p-value is, the more probable is the alternative hypothesis becomes.
In other words a small p-value shows that the model has a meaningful statistical relationship.

The R squared is fraction of variance explained by the model. In other words, if this value is large, the model gives a good fit and there is little or no need to search for additional explaining variables.

### iv) Understanding the fitted model output



The multiple R squared of the model is 0.2182. I would have expected to have a little higher R squared. This gives an impression there would be still more descriptive variables "out there" to be added to the model. Still the R squared is positive so the model is better than fitting a straight horizontal line.

In conclusion, this model can be understood so that the **attitude** variable has a very significant relationship with the exam score, having a p value < 0.001. This means that
this alternative hypothesis is very string.

Also the **age** and **strategic learning mode** have somewhat significant relation on the output, since the probability of null hypothesis (ie. no significance) is less than 0.1.
It seems the higher age would predict slightly lower exam score, and using strategic learning
would produce some extra exam points. Such effect might be worth of further study.

### v) Some useful plots on the fitted model

R has this convenience function for plotting lm() models:
```{r}
plot(my_model2,c(1,2,5))
```

These graphs help us understand and evaluate, how appropriate our 
statistical model for estimating the effect of student's age, attitude towards
statistics and their adaptation of strategic learning methods.

The **Residuals vs fitted values** graph shows a general visualization how well this
linear model fits the data. If the residuals are uniformly scattered from the
low to high end of fitted values, we can be pretty comfortable with these coefficients.

The **Normal Q-Q plot** is a quick visualization, on how well the error residuals distribution
follows the Gaussian distribution. If most of the values are laid among the dotted diagonal,
we could be assured there is no further systematic "explaining" variable to be examined.

The **Residuals vs leverage** graph is a useful tool for finding *outliers* in a model.
An *outlier* is a data point that its output value is exceptionally far from the estimate
that is produced from the variables. 
In other words, any points over the dotted red line would have high influence on the
fitted model. Probably any of the points do not have specifically high influence
on our model coefficients, and the residuals are quite normally distributed.







